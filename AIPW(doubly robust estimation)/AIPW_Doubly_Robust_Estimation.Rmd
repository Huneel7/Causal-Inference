---
title: "Augmented Inverse Propensity Weighting"
subtitle: "(Doubly Robust Estimation)"
output: pdf_document
---

From IP Weighting part, we saw how to unbiasedly estimate the average treatment effect through inverse probability weighting. Our goal is still to estimate the average causal treatment effect from observational data, adjusting for confounding. This time, we are introducing the Augmented inverse Propensity Weighting (AIPW) method to estimate the average treatment effect. This method is also known as doubly robust estimation. This is because AIPW estimator remains consistent for the average treatment effect if either the propensity score model or the outcome regression is misspecified but the other is properly specified.

## Regression Model & Adjustment by Regression Modeling

Let Y is the outcome, A is the treatment, and L is the set of all measured covariates (confounders). 

Regression of Y on A and L: 

We can identify the regression $E[Y \mid A, L]$, as this depends on the observed data.

For continuous outcome, $E[Y \mid A, L] = \beta_0 + \beta_1A + L^{T}\beta_2$

$E[Y \mid A = 1, L]$ is the regression among treated and $E[Y \mid A = 0, L]$ among control.
$\\$

If we average over all possible values of L, we get the following result:

$E(E[Y \mid A = 1, L])$ 

$= \ E(E[Y^{1} \mid A = 1, L]) \ \ $ (by consistency)

$= \ E(E[Y^{1} \mid L]) \ \ \ \ \ \ \ \ \ \ \ $ (by conditional exchangeabiltiy under no unmeasured confounders, $Y^{a} \perp\!\!\!\perp A \mid L$)

$= \ E[Y^{1}]$
$\\$

Similarly, $E(E[Y \mid A = 0, L]) = E[Y^{0}]$
$\\$

Thus, $\triangle = E[Y^{1}]-E[Y^{0}]$ 

$= \ E(E[Y^{1} \mid A = 1, L])- E[Y^{1} \mid A = 0, L])$

$= \ \beta_0 + \beta_1 + L^{T}\beta_2 - \beta_0 + L^{T}\beta_2$

$= \ \beta_1$
$\\$

$\therefore \ \hat{\triangle} = \hat{\beta_1}$
$\\$

\newpage

## Propensity Score Model & Adjustment by Inverse Weighting

Propensity score:
$P(A=1 \mid L) = E[I(A=1) \mid L] = E[A \mid L]$

Under no unmeasured confounders, $Y^{a} \perp\!\!\!\perp A \mid E[A \mid L]$
$\\$

Fitting a logistic regression for propensity score:

$P(A=1 \mid L) = \frac{exp{(\alpha_0 + L^{T}\alpha_1)}}{1+exp{(\alpha_0 + L^{T}\alpha_1)}}$
$\\$

Rather than use the difference of simple averages $\overline{Y}^{(1)} - \overline{Y}^{(0)}$,
estimate $\triangle$ by the difference of inverse propensity score weighted averages:

$\hat{\triangle}_{IPW} = \frac{1}{n}\sum_{i=1}^{n}\frac{A_{i}Y_{i}}{\hat{P}(A=1 \mid L{i})} - \frac{1}{n}\sum_{i=1}^{n}\frac{(1-A_{i})Y_{i}}{1-\hat{P}(A=1 \mid L_{i})}$
$\\$

If ${\hat{P}(A=1 \mid L)} = P(A=1 \mid L)$, the true propensity score:

$E\left[\frac{AY}{P(A=1 \mid L)}\right]$

$= \ E\left[\frac{AY^{1}}{P(A=1 \mid L)}\right] \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ $ (by $AY = A(Y^{1}A + Y^{0}(1-A)) = AY^{1}$ for $A^2 = A$)
 
$= \ E\left(E\left[\frac{AY^{a}}{P(A=1 \mid L)} \mid Y^{a}, L \right]\right)$

$= \ E\left(\frac{Y^{1}}{P(A=1 \mid L)}E\left[A \mid Y^{a}, L \right]\right)$

$= \ E\left(\frac{Y^{1}}{P(A=1 \mid L)}E\left[A \mid L \right]\right) \ \ \ \ \ $ (under no unmeasured confounders)

$= \ E\left(\frac{Y^{1}}{E(A \mid L)}E\left[A \mid L \right]\right)$

$= \ E[Y^{1}]$
$\\$

Similarly, $E\left[\frac{(1-A)Y}{1-P(A=1 \mid L)}\right] = E[Y^{0}]$
$\\$

$\therefore \ \hat{\triangle}_{IPW}$ estimates $\triangle$ $(E[Y^1]-E[Y^0])$ when the propensity model is identical to the true propensity score.

## Doubly Robust Estimator

$\hat{\triangle}_{DR} = \frac{1}{n}\sum_{i=1}^{n} \left[\frac{A_{i}Y_{i}}{\hat{P}(A=1 \mid L{i})}-\frac{A_{i}-\hat{P}(A=1 \mid L{i})}{\hat{P}(A=1 \mid L{i})}\hat{E}(Y_{i} \mid A_{i}=1, L_{i})\right] -  \frac{1}{n}\sum_{i=1}^{n} \left[\frac{(1-A_{i})Y_{i}}{1-\hat{P}(A=1 \mid L_{i})} + \frac{A_{i}-\hat{P}(A=1 \mid L{i})}{1-\hat{P}(A=1 \mid L{i})}\hat{E}(Y_{i} \mid A_{i}=0, L_{i})\right]$
$\\$

$\phantom{space} = \hat{\mu}_{1,DR} - \hat{\mu}_{0,DR}$

## What does $\hat{\mu}_{1,DR}$ estimate?

$= \ E\left[\frac{AY}{P(A=1 \mid L)}-\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}E(Y \mid A=1, L)\right]$

$= \ E\left[\frac{AY^{1}}{P(A=1 \mid L)}-\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}E(Y \mid A=1, L)\right]$

$= \ E\left[Y^{1} + \frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\right] \ \ $ (by algebra)  

$= \ E[Y^{1}] + E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\right]$

\newpage
## Does $\hat{\triangle}_{DR}$ estimate $\triangle$?

$\hat{\mu}_{1,DR}$ estimates $\\$ $\phantom{spacespace}$ $E[Y^{1}] + E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\right] \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (*)$

In order for $\hat{\mu}_{1,DR}$ to estimate $E[Y_{1}]$, the second term in $(*)$ must be $0$.

It is 0 when either **postulated propensity score model** or **postulated regression model** is correct.
$\\$

**Scenario 1: Propensity score model $P(A=1 \mid L)$ is correct**

$E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\right]$

$= \ E\left(E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\mid Y^1, L\right] \right)$

$= \ E\left(\{Y^{1}-E(Y \mid A=1, L)\}E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)} \mid Y^1, L\right]\right)$

$= \ E\left(\{Y^{1}-E(Y \mid A=1, L)\}E\left[\frac{A-E(A \mid L)}{E(A \mid L)} \mid Y^1, L\right]\right)$

$= \ E\left(\{Y^{1}-E(Y \mid A=1, L)\}E\left[\frac{E[A \mid Y^1, L]-E(A \mid L)}{E(A \mid L)}\right]\right)$

$= \ E\left(\{Y^{1}-E(Y \mid A=1, L)\}E\left[\frac{E[A \mid L]-E(A \mid L)}{E(A \mid L)}\right]\right) \ \ $ (under no unmeasured confounders)

$= \ 0$
$\\$

$\therefore \ \hat{\mu}_{1,DR} \ \ \text{estimates} \ E[Y^1]$ and similarly $\hat{\mu}_{0,DR} \ \ \text{estimates} \ E[Y^0]$ when propensity score model is correct
$\\$

**Scenario 2: Regression model $E(Y \mid A=1, L)$ is correct** 

$E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\right]$

$= \ E\left(E\left[\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{Y^{1}-E(Y \mid A=1, L)\}\mid A, L\right] \right)$

$= \ E\left(\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}E\left[\{Y^{1}-E(Y \mid A=1, L)\}\mid A, L\right] \right)$

$= \ E\left(\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}E\left[\{Y^{1}-E(Y \mid A=1, L)\}\mid A, L\right] \right)$

$= \ E\left(\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{E[Y^{1} \mid A, L]-E(Y \mid A=1, L)\} \right)$

$= \ E\left(\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{E[Y^{1} \mid A, L]-E(Y^1 \mid A=1, L)\} \right)$

$= \ E\left(\frac{A-P(A=1 \mid L)}{P(A=1 \mid L)}\{E[Y^{1} \mid L]-E(Y^1 \mid L)\} \right) \ \ $ (under no unmeasured confounders)

$= \ 0$
$\\$

$\therefore \ \hat{\mu}_{1,DR} \ \ \text{estimates} \ E[Y^1]$ and similarly $\hat{\mu}_{0,DR} \ \ \text{estimates} \ E[Y^0]$ when regression model is correct
$\\$

Therefore, $\hat{\triangle}_{DR}$ estimates $\triangle$!

\newpage

```{r, echo = FALSE, message = FALSE}
library(tidyverse)
library(simstudy)
library(CausalGAM)
library(broom)
library(performance)
```

## Generating  data with true ATE = 0.5
```{r}
pre_data <- defData(varname = "L1", formula = "0", variance = 1,
                dist = "normal")
pre_data <- defData(pre_data, varname = "L2", formula = "0", variance = 1,
                dist = "normal")
pre_data <- defData(pre_data, varname = "L3", formula = "0;1",
                dist = "uniform")
pre_data <- defData(pre_data, varname = "A", formula = " -1.3 + 1.2*L2",
                dist = "binary", link = "logit")
pre_data <- defData(pre_data, varname = "Y", formula = "L1 + 0.5*L2 + 0.5*A", 
                    variance = 0.5, dist = "normal")
set.seed(7777)
df <- genData(500, pre_data)
```

## Propensity Model
```{r, message=FALSE, fig.width=5, fig.height=3}
logistic_model <- glm(A ~ L2, family = binomial, data = df)
logistic_model %>% broom::tidy()
ggplot(df, aes(L2, A)) + geom_point() + 
  geom_smooth(method = "glm", se = FALSE, method.args = list(family = "binomial"))
```
\newpage
## Regression Model
```{r}
linear_model <- lm(Y ~ L1 + L2, data = df)
linear_model %>% broom::tidy()
check_model(linear_model, 
            check = c("linearity", "qq", "normality", "homogeneity"))
```


\newpage
## Case when regression model is correct, but propensity model is incorrect
```{r}
ATE.out <- estimate.ATE(pscore.formula = A ~ s(L1),
                        pscore.family = binomial,
                        outcome.formula.t = Y ~ s(L1) + s(L2),
                        outcome.formula.c = Y ~ s(L1) + s(L2),
                        outcome.family = gaussian,
                        treatment.var = "A", data = df,
                        divby0.action = "t", divby0.tol = 0.001,
                        var.gam.plot = FALSE, nboot = 50)
print(ATE.out)
```

\newpage
## Case when propensity model is correct, but regression model is incorrect
```{r}
ATE.out_2 <- estimate.ATE(pscore.formula = A ~ s(L2),
                        pscore.family = binomial,
                        outcome.formula.t = Y ~ s(L1) + s(L3),
                        outcome.formula.c = Y ~ s(L1) + s(L3),
                        outcome.family = gaussian,
                        treatment.var = "A", data = df,
                        divby0.action = "t", divby0.tol = 0.001,
                        var.gam.plot = FALSE, nboot = 50)
print(ATE.out_2)
```
