---
title: "Inverse Probability Weighting"
output: pdf_document
---

<style type="text/css">

body{
   font-size: 20px;
}

h1.title {
  font-size: 25px;
  color: DarkRed;
}

h1 { /* Header 1 */
  font-size: 20px;
  color: DarkBlue;
}

</style>

```{r, echo = FALSE, message = FALSE}
library(dagitty)
library(ggdag)
library(dplyr)
library(ggplot2)
library(tidyverse)
library(geepack)
library(broom)
```

In backdoor criterion part, we learned that PA(X) always satisfies backdoor criterion. Hence, we can represent postintervention causal effect as such:

$P(Y=y \mid do(X=x))$ = $\sum_{z}^{} P(Y=y \mid X=x,PA=z)P(PA=z)$

Modifying this equation, we can obtain the following:

$P(Y=y \mid do(X=x))$ = $\sum_{z}^{}\frac{ P(Y=y, X=x, PA=z)}{P(X = x \mid PA=z)}$

The factor ${P(X = x \mid PA=z)}$ in the denominator is known as the "propensity score."

$\\$

Now, assume we have a graphical model with an intervention, do(X=x), on the model. 


```{r, echo = FALSE, message = FALSE, fig.width=10, fig.height=3.2}
g <- dagitty('dag {
    X [pos="0,0"]
    Y [pos="2,0"]
    Z [pos="1,1"]
    X -> Y 
    Z -> Y
}')

ggdag(g) + theme_dag_blank()
```


$P(Y=y \mid do(X=x))$ 

$= \ P_{m}(Y=y \mid X=x) \ \ $ (by definition) 

$= \ \sum_{z}^{} P_{m}(Y=y \mid X=x,Z=z)P(Z=z \mid X=x) \ \ $ (Bayes' rule)

$= \ \sum_{z}^{} P_{m}(Y=y \mid X=x,Z=z)P(Z=z) \ \ $ ($X \perp\!\!\!\perp Z$)

$= \ \sum_{z}^{} P(Y=y \mid X=x,Z=z)P(Z=z) \ \ $ (invariance relations)

$= \ \sum_{z}^{} \frac{P(Y=y \mid X=x,Z=z)P(X = x \mid Z=z)P(Z=z)}{P(X = x \mid Z=z)}$

$= \ \sum_{z}^{} \frac{ P(Y=y, X=x,Z=z)}{P(X = x \mid Z=z)}$

$\\$

From this result, we can see that postintervention causal effect can be computed by multiplying the pretreatment distribution of (X,Y,Z) by a factor $1/P(X = x \mid Z=z)$, propensity score. Namely, each case $(Y=y, X=x, Z=z)$in the population should boost its probability by the inverse conditional probability of assignment to a treatment condition given a set of observed covariates. This is the reason why this method is called "inverse probability weighting."


\newpage

## Importing data
```{r, message = FALSE, warning = FALSE}
nhefs <- read_csv("nhefs.csv")

nhefs_uncensored <-
  nhefs %>%
  mutate(cens = ifelse(is.na(wt82), 1, 0)) %>%
  relocate(cens, wt82) %>%
  filter(!is.na(wt82))
```

## Estimation of ip weights via a logistic model
```{r}
propensity_model <- glm(
  qsmk ~ sex + race + age + I(age ^ 2) +
    as.factor(education) + smokeintensity +
    I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) +
    as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2),
  family = binomial(),
  data = nhefs_uncensored
)
```

## Computing propensity score. Note that Pr[A=0|L] = 1-Pr[A=1|L]
```{r, message = FALSE, warning = FALSE}
p.qsmk.obs <-
  ifelse(nhefs_uncensored$qsmk == 0,
         1 - predict(propensity_model, type = "response"),
         predict(propensity_model, type = "response"))

nhefs_uncensored <-
  nhefs_uncensored %>%
  mutate(w = 1/p.qsmk.obs, #inverse propensity score (weight)
         ps = p.qsmk.obs) #propensity score

nhefs_uncensored %>%
  ggplot(aes(ps, group = qsmk, fill = factor(qsmk))) + 
  geom_histogram(alpha = 0.5, position = "identity") +
  scale_fill_manual(values = c("skyblue", "orange"), 
                    labels = c("0 = Non-quitters", "Smoking quitters")) +
  labs(x = "Propensity Score",
       title = "Distribution of Propensity Score for Quitters vs Non-quitters")
```

## Computing a coefficient (Beta 1) of the marginal structural model and its Confidence Interval
```{r}
msm.w <- geeglm(
  wt82_71 ~ qsmk,
  data = nhefs_uncensored,
  weights = w,
  id = seqn,
  corstr = "independence"
)

beta <- coef(msm.w)
SE <- coef(summary(msm.w))[, 2]
Lower_CI <- beta - qnorm(0.975) * SE
Upper_CI <- beta + qnorm(0.975) * SE
cbind(beta, Lower_CI, Upper_CI) 
```



\newpage

\newpage

# Reference

Pearl, J., Glymour, M., &amp; Jewell, N. P. (2019). Causal inference in statistics a primer. Wiley. 
$\\$

HernÃ¡n MA, Robins JM (2020). Causal Inference: What If. Boca Raton: Chapman & Hall/CRC.
